{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is categorical cross-entropy and explain why it's a much better cost function than accuracy, and why it might sometimes be a better choice than mean squared error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Categorical cross-entropy is a loss function which follows the following formula:$\\sum_{i=1}^ny_i ⋅ log\\widehat{ y_i}$ where $\\widehat{y_i}$\n",
    "is the $i$-th scalar value in the model output, $y_i$ is the corresponding target value, and n is the number of scalar values in the model output. Categorical cross-entropy is used for single labelled categorisation i.e. when an example can belong to one class only, because of this it is often used in classification tasks. Categorical cross-entropy is much better for classification tasks while mean squared error is much better for regression problems. This is due to the fact that for classification tasks the decision boundary is often large and if you used MSE in this case then it would not punish the misclassifications enough this is also a reason why it is sometimes a better choice than mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "df = pd.read_csv('MNIST_train.csv')\n",
    "\n",
    "#Inspect the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset the column 'label' is the target variable, in this case it tells us which number is actually in the picture. The rest of the columns are the predictors and are used to show the image of the number. Knowing this lets split the data into the targets and predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into target and predictors\n",
    "X = df['label'].values\n",
    "target = keras.utils.to_categorical(X,10)\n",
    "predictors = df.drop('label',axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's split the data into test data and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors,target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 1s 1ms/step - loss: 3.4712 - accuracy: 0.7498 - val_loss: 0.9887 - val_accuracy: 0.7890\n",
      "394/394 [==============================] - 0s 472us/step - loss: 0.9378 - accuracy: 0.7979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.937828779220581, 0.7979364991188049]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,y_train,validation_split=0.3)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's define what the loss function actually does. The loss function is used to optimize a machine learning algorithm. The loss is calculated on both training and validation and its interpretation is based on how well the model is doing in these two sets. It is the sum of errors made for each example. The lower the loss value the better a model is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see when training for one epoch we have a loss of 0.9378 and an accuracy of 79.79% . Let's see how the accuracy and loss is effected when we run the exact same model but this time with one hidden layer instead of two. Initially I would think the accuracy and loss would be worse as a more complex model usually achieves better loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/644 [..............................] - ETA: 0s - loss: 138.5491 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 4.8766 - accuracy: 0.6020 - val_loss: 1.1899 - val_accuracy: 0.6810\n",
      "394/394 [==============================] - 0s 487us/step - loss: 1.1241 - accuracy: 0.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1241025924682617, 0.6874603033065796]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,y_train,validation_split=0.3)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our guess was correct. The loss is now 1.1241 and the accuracy is 68.75% which is worse than before. Let's take a look at the effect of running the model for more than one epoch and see if this has any effect on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running for more Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "644/644 [==============================] - 1s 965us/step - loss: 2.8272 - accuracy: 0.5761 - val_loss: 0.9627 - val_accuracy: 0.7347\n",
      "Epoch 2/20\n",
      "644/644 [==============================] - 1s 824us/step - loss: 0.7135 - accuracy: 0.8122 - val_loss: 0.6311 - val_accuracy: 0.8518\n",
      "Epoch 3/20\n",
      "644/644 [==============================] - 1s 961us/step - loss: 0.4988 - accuracy: 0.8734 - val_loss: 0.5374 - val_accuracy: 0.8814\n",
      "Epoch 4/20\n",
      "644/644 [==============================] - 1s 896us/step - loss: 0.3968 - accuracy: 0.8979 - val_loss: 0.4712 - val_accuracy: 0.8952\n",
      "Epoch 5/20\n",
      "644/644 [==============================] - 1s 905us/step - loss: 0.3635 - accuracy: 0.9074 - val_loss: 0.5209 - val_accuracy: 0.8841\n",
      "Epoch 6/20\n",
      "644/644 [==============================] - 1s 903us/step - loss: 0.3430 - accuracy: 0.9135 - val_loss: 0.4958 - val_accuracy: 0.8848\n",
      "Epoch 7/20\n",
      "644/644 [==============================] - 1s 820us/step - loss: 0.3281 - accuracy: 0.9190 - val_loss: 0.4682 - val_accuracy: 0.8988\n",
      "Epoch 8/20\n",
      "644/644 [==============================] - 1s 829us/step - loss: 0.3196 - accuracy: 0.9216 - val_loss: 0.4978 - val_accuracy: 0.9024\n",
      "Epoch 9/20\n",
      "644/644 [==============================] - 1s 818us/step - loss: 0.2870 - accuracy: 0.9264 - val_loss: 0.4311 - val_accuracy: 0.9024\n",
      "Epoch 10/20\n",
      "644/644 [==============================] - 1s 931us/step - loss: 0.2782 - accuracy: 0.9299 - val_loss: 0.5755 - val_accuracy: 0.8946\n",
      "Epoch 11/20\n",
      "644/644 [==============================] - 1s 840us/step - loss: 0.2592 - accuracy: 0.9369 - val_loss: 0.5228 - val_accuracy: 0.9034\n",
      "Epoch 12/20\n",
      "644/644 [==============================] - 1s 891us/step - loss: 0.2597 - accuracy: 0.9354 - val_loss: 0.5177 - val_accuracy: 0.9176\n",
      "Epoch 13/20\n",
      "644/644 [==============================] - 1s 823us/step - loss: 0.2435 - accuracy: 0.9386 - val_loss: 0.4437 - val_accuracy: 0.9037\n",
      "Epoch 14/20\n",
      "644/644 [==============================] - 1s 823us/step - loss: 0.2215 - accuracy: 0.9447 - val_loss: 0.4366 - val_accuracy: 0.9145\n",
      "Epoch 15/20\n",
      "644/644 [==============================] - 1s 812us/step - loss: 0.2209 - accuracy: 0.9439 - val_loss: 0.4052 - val_accuracy: 0.9198\n",
      "Epoch 16/20\n",
      "644/644 [==============================] - 1s 818us/step - loss: 0.2339 - accuracy: 0.9410 - val_loss: 0.4442 - val_accuracy: 0.9167\n",
      "Epoch 17/20\n",
      "644/644 [==============================] - 1s 849us/step - loss: 0.2061 - accuracy: 0.9464 - val_loss: 0.4311 - val_accuracy: 0.9152\n",
      "Epoch 18/20\n",
      "644/644 [==============================] - 1s 823us/step - loss: 0.1925 - accuracy: 0.9490 - val_loss: 0.4353 - val_accuracy: 0.9256\n",
      "Epoch 19/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1896 - accuracy: 0.9512 - val_loss: 0.4212 - val_accuracy: 0.9236\n",
      "Epoch 20/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1816 - accuracy: 0.9516 - val_loss: 0.4373 - val_accuracy: 0.9288\n",
      "  1/394 [..............................] - ETA: 0s - loss: 0.0265 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "394/394 [==============================] - 0s 612us/step - loss: 0.4201 - accuracy: 0.9306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42005637288093567, 0.9305555820465088]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,y_train,validation_split=0.3, epochs=20)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the model with two hidden layers for 20 epochs we can see the loss is now 0.4201 and the accuracy is 93.06%. This is a significance improvement from before, lets see if the model with one hidden layer follows the same pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 4.4832 - accuracy: 0.5455 - val_loss: 1.2609 - val_accuracy: 0.6272\n",
      "Epoch 2/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 1.0175 - accuracy: 0.7103 - val_loss: 0.8582 - val_accuracy: 0.7642\n",
      "Epoch 3/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.7064 - accuracy: 0.8069 - val_loss: 0.7178 - val_accuracy: 0.8211\n",
      "Epoch 4/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.5463 - accuracy: 0.8499 - val_loss: 0.5905 - val_accuracy: 0.8516\n",
      "Epoch 5/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4729 - accuracy: 0.8691 - val_loss: 0.6215 - val_accuracy: 0.8616\n",
      "Epoch 6/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4087 - accuracy: 0.8891 - val_loss: 0.5329 - val_accuracy: 0.8690\n",
      "Epoch 7/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3582 - accuracy: 0.8992 - val_loss: 0.5022 - val_accuracy: 0.8856\n",
      "Epoch 8/20\n",
      "644/644 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.91 - 1s 1ms/step - loss: 0.3208 - accuracy: 0.9104 - val_loss: 0.4670 - val_accuracy: 0.8874\n",
      "Epoch 9/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2934 - accuracy: 0.9162 - val_loss: 0.4374 - val_accuracy: 0.9101\n",
      "Epoch 10/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2614 - accuracy: 0.9236 - val_loss: 0.4266 - val_accuracy: 0.9098\n",
      "Epoch 11/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2647 - accuracy: 0.9263 - val_loss: 0.3959 - val_accuracy: 0.9103\n",
      "Epoch 12/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2427 - accuracy: 0.9299 - val_loss: 0.3919 - val_accuracy: 0.9120\n",
      "Epoch 13/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2252 - accuracy: 0.9367 - val_loss: 0.3979 - val_accuracy: 0.9149\n",
      "Epoch 14/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2204 - accuracy: 0.9362 - val_loss: 0.3961 - val_accuracy: 0.9138\n",
      "Epoch 15/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2219 - accuracy: 0.9358 - val_loss: 0.3774 - val_accuracy: 0.9111\n",
      "Epoch 16/20\n",
      "644/644 [==============================] - 1s 2ms/step - loss: 0.2123 - accuracy: 0.9399 - val_loss: 0.4021 - val_accuracy: 0.9144\n",
      "Epoch 17/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1941 - accuracy: 0.9426 - val_loss: 0.4107 - val_accuracy: 0.9164\n",
      "Epoch 18/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1900 - accuracy: 0.9460 - val_loss: 0.4066 - val_accuracy: 0.9185\n",
      "Epoch 19/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1991 - accuracy: 0.9434 - val_loss: 0.4140 - val_accuracy: 0.9194\n",
      "Epoch 20/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.9470 - val_loss: 0.4328 - val_accuracy: 0.9205\n",
      "394/394 [==============================] - 0s 718us/step - loss: 0.3921 - accuracy: 0.9253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3921052813529968, 0.9253174662590027]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,y_train, validation_data=(X_test,y_test) ,validation_split=0.3, epochs=20)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy and loss after 20 epochs have also improved for the model with one hidden layer. This model now has a loss of 0.3921 and an accuracy of 92.53% where it only had a loss of 1.1241 and an accuracy of 68.75% when run for one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models usually converge to a certain performance and it is often the case where the time taken to run an extra epoch is not worth the small improvement in performance. So this brings up the question of what is a good number of epochs to run your model for? In keras there is a function called EarlyStopping. This function will stop running epochs when the validation loss doesn't improve for n epochs. In this case I will specify the patience(n) equal to 2(for experimental purpose) which means it will stop running when the validation loss hasn't improved for two epochs. I will also specify epochs = 50, it is okay to specify it as a large number as it is going to be stopped anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 3.0903 - accuracy: 0.6829 - val_loss: 0.9757 - val_accuracy: 0.7681\n",
      "Epoch 2/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.6551 - accuracy: 0.8328 - val_loss: 0.6434 - val_accuracy: 0.8490\n",
      "Epoch 3/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4958 - accuracy: 0.8763 - val_loss: 0.5536 - val_accuracy: 0.8764\n",
      "Epoch 4/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4354 - accuracy: 0.8912 - val_loss: 0.4914 - val_accuracy: 0.8850\n",
      "Epoch 5/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3742 - accuracy: 0.9071 - val_loss: 0.5107 - val_accuracy: 0.8857\n",
      "Epoch 6/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3403 - accuracy: 0.9155 - val_loss: 0.4518 - val_accuracy: 0.9090\n",
      "Epoch 7/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3099 - accuracy: 0.9224 - val_loss: 0.4957 - val_accuracy: 0.9015\n",
      "Epoch 8/50\n",
      "644/644 [==============================] - 1s 954us/step - loss: 0.3072 - accuracy: 0.9252 - val_loss: 0.4865 - val_accuracy: 0.9057\n",
      "394/394 [==============================] - 0s 466us/step - loss: 0.4364 - accuracy: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43639492988586426, 0.9123809337615967]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model(specify epochs=30 here as it will stop regardless)\n",
    "model.fit(X_train,y_train,validation_split=0.3, epochs=50,callbacks=[early_stopping_monitor])\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see for our model with two hidden layers it stopped running after 8 epochs. Although the accuracy is now worse than before if we were running a model with a few thousand hidden layers the difference in the run time between these 8 epochs and the 20 epochs which gave us the better accuracy may not be worth the small increase in accuracy. One other alternative method you could play around with different values for the patience which could give you a better accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 4.6416 - accuracy: 0.6613 - val_loss: 1.1333 - val_accuracy: 0.7188\n",
      "Epoch 2/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.8826 - accuracy: 0.7805 - val_loss: 0.8836 - val_accuracy: 0.7762\n",
      "Epoch 3/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.6305 - accuracy: 0.8454 - val_loss: 0.7475 - val_accuracy: 0.8295\n",
      "Epoch 4/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.5088 - accuracy: 0.8692 - val_loss: 0.6967 - val_accuracy: 0.8510\n",
      "Epoch 5/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4535 - accuracy: 0.8829 - val_loss: 0.6306 - val_accuracy: 0.8730\n",
      "Epoch 6/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3895 - accuracy: 0.8988 - val_loss: 0.7013 - val_accuracy: 0.8678\n",
      "Epoch 7/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3467 - accuracy: 0.9078 - val_loss: 0.5635 - val_accuracy: 0.8800\n",
      "Epoch 8/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3236 - accuracy: 0.9119 - val_loss: 0.4978 - val_accuracy: 0.8983\n",
      "Epoch 9/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2987 - accuracy: 0.9163 - val_loss: 0.5169 - val_accuracy: 0.8984\n",
      "Epoch 10/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2930 - accuracy: 0.9223 - val_loss: 0.4527 - val_accuracy: 0.9033\n",
      "Epoch 11/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2581 - accuracy: 0.9289 - val_loss: 0.4597 - val_accuracy: 0.9090\n",
      "Epoch 12/50\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2600 - accuracy: 0.9275 - val_loss: 0.4712 - val_accuracy: 0.9086\n",
      "394/394 [==============================] - 0s 597us/step - loss: 0.3984 - accuracy: 0.9171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3984324336051941, 0.9171428680419922]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model(specify epochs=30 here as it will stop regardless)\n",
    "model.fit(X_train,y_train,validation_split=0.3, epochs=50,callbacks=[early_stopping_monitor])\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with one hidden layer ran for 12 epochs compared to the model with two hidden layers which ran for 8 epochs. Although the model with one hidden layer got a better accuracy in this case it was run for 4 more epochs and was only an increase in .47% accuracy. To conclude I believe the model with two outputs converges quicker to a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Less Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we got our best accuracies when we ran our models for 20 epochs. Let's do the same for both a model with one hidden layer and a model with two hidden layers. Let's firstly decrease the number of neurons in these layers and see how this effects the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "644/644 [==============================] - 1s 934us/step - loss: 2.4149 - accuracy: 0.5336 - val_loss: 1.0584 - val_accuracy: 0.6757\n",
      "Epoch 2/20\n",
      "644/644 [==============================] - 1s 778us/step - loss: 0.8848 - accuracy: 0.7289 - val_loss: 0.8069 - val_accuracy: 0.7861\n",
      "Epoch 3/20\n",
      "644/644 [==============================] - 1s 784us/step - loss: 0.6815 - accuracy: 0.7980 - val_loss: 0.6776 - val_accuracy: 0.8066\n",
      "Epoch 4/20\n",
      "644/644 [==============================] - 1s 947us/step - loss: 0.5811 - accuracy: 0.8274 - val_loss: 0.6654 - val_accuracy: 0.8217\n",
      "Epoch 5/20\n",
      "644/644 [==============================] - 1s 937us/step - loss: 0.5382 - accuracy: 0.8441 - val_loss: 0.5904 - val_accuracy: 0.8477\n",
      "Epoch 6/20\n",
      "644/644 [==============================] - 1s 948us/step - loss: 0.4936 - accuracy: 0.8567 - val_loss: 0.5717 - val_accuracy: 0.8476\n",
      "Epoch 7/20\n",
      "644/644 [==============================] - 1s 953us/step - loss: 0.4827 - accuracy: 0.8649 - val_loss: 0.5456 - val_accuracy: 0.8452\n",
      "Epoch 8/20\n",
      "644/644 [==============================] - 1s 940us/step - loss: 0.4403 - accuracy: 0.8699 - val_loss: 0.5323 - val_accuracy: 0.8676\n",
      "Epoch 9/20\n",
      "644/644 [==============================] - 1s 944us/step - loss: 0.4091 - accuracy: 0.8800 - val_loss: 0.5259 - val_accuracy: 0.8681\n",
      "Epoch 10/20\n",
      "644/644 [==============================] - 1s 944us/step - loss: 0.3777 - accuracy: 0.8838 - val_loss: 0.4830 - val_accuracy: 0.8639\n",
      "Epoch 11/20\n",
      "644/644 [==============================] - 1s 946us/step - loss: 0.3801 - accuracy: 0.8841 - val_loss: 0.5529 - val_accuracy: 0.8472\n",
      "Epoch 12/20\n",
      "644/644 [==============================] - 1s 949us/step - loss: 0.3618 - accuracy: 0.8913 - val_loss: 0.5603 - val_accuracy: 0.8866\n",
      "Epoch 13/20\n",
      "644/644 [==============================] - 1s 942us/step - loss: 0.3341 - accuracy: 0.9019 - val_loss: 0.5037 - val_accuracy: 0.8820\n",
      "Epoch 14/20\n",
      "644/644 [==============================] - 1s 946us/step - loss: 0.3190 - accuracy: 0.9067 - val_loss: 0.4505 - val_accuracy: 0.8967\n",
      "Epoch 15/20\n",
      "644/644 [==============================] - 1s 946us/step - loss: 0.2983 - accuracy: 0.9133 - val_loss: 0.4234 - val_accuracy: 0.9045\n",
      "Epoch 16/20\n",
      "644/644 [==============================] - 1s 940us/step - loss: 0.2842 - accuracy: 0.9155 - val_loss: 0.4270 - val_accuracy: 0.8964\n",
      "Epoch 17/20\n",
      "644/644 [==============================] - 1s 944us/step - loss: 0.2665 - accuracy: 0.9215 - val_loss: 0.3900 - val_accuracy: 0.9053\n",
      "Epoch 18/20\n",
      "644/644 [==============================] - 1s 947us/step - loss: 0.2685 - accuracy: 0.9222 - val_loss: 0.4403 - val_accuracy: 0.9005\n",
      "Epoch 19/20\n",
      "644/644 [==============================] - 1s 950us/step - loss: 0.2462 - accuracy: 0.9284 - val_loss: 0.4161 - val_accuracy: 0.9012\n",
      "Epoch 20/20\n",
      "644/644 [==============================] - 1s 852us/step - loss: 0.2227 - accuracy: 0.9349 - val_loss: 0.4577 - val_accuracy: 0.9070\n",
      "394/394 [==============================] - 0s 421us/step - loss: 0.4508 - accuracy: 0.9083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4507608413696289, 0.9082539677619934]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(30,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(30,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,y_train,validation_split=0.3, epochs=20)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a loss of 0.4201 when we used 50 neurons and a loss of 0.4508 when we used 30 neurons. Both the loss and the accuracy is better when using 50 neurons. The accuracy is 2.24% better which is quite a lot. Let's see if this follows a similar pattern for our model with only one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 3.7065 - accuracy: 0.4014 - val_loss: 1.4731 - val_accuracy: 0.5342\n",
      "Epoch 2/20\n",
      "644/644 [==============================] - 1s 903us/step - loss: 1.2401 - accuracy: 0.6056 - val_loss: 1.2052 - val_accuracy: 0.6324\n",
      "Epoch 3/20\n",
      "644/644 [==============================] - 1s 917us/step - loss: 0.9718 - accuracy: 0.6738 - val_loss: 1.0506 - val_accuracy: 0.6875\n",
      "Epoch 4/20\n",
      "644/644 [==============================] - 1s 900us/step - loss: 0.8345 - accuracy: 0.7194 - val_loss: 0.9361 - val_accuracy: 0.7259\n",
      "Epoch 5/20\n",
      "644/644 [==============================] - 1s 914us/step - loss: 0.7342 - accuracy: 0.7531 - val_loss: 0.8272 - val_accuracy: 0.7456\n",
      "Epoch 6/20\n",
      "644/644 [==============================] - 1s 944us/step - loss: 0.6931 - accuracy: 0.7696 - val_loss: 0.8280 - val_accuracy: 0.7619\n",
      "Epoch 7/20\n",
      "644/644 [==============================] - 1s 928us/step - loss: 0.6473 - accuracy: 0.7858 - val_loss: 0.7471 - val_accuracy: 0.7978\n",
      "Epoch 8/20\n",
      "644/644 [==============================] - 1s 950us/step - loss: 0.5910 - accuracy: 0.8086 - val_loss: 0.7975 - val_accuracy: 0.8028\n",
      "Epoch 9/20\n",
      "644/644 [==============================] - 1s 801us/step - loss: 0.5411 - accuracy: 0.8264 - val_loss: 0.8094 - val_accuracy: 0.8120\n",
      "Epoch 10/20\n",
      "644/644 [==============================] - 1s 918us/step - loss: 0.5089 - accuracy: 0.8438 - val_loss: 0.6987 - val_accuracy: 0.8562\n",
      "Epoch 11/20\n",
      "644/644 [==============================] - 0s 753us/step - loss: 0.4893 - accuracy: 0.8609 - val_loss: 0.6705 - val_accuracy: 0.8556\n",
      "Epoch 12/20\n",
      "644/644 [==============================] - 0s 759us/step - loss: 0.4608 - accuracy: 0.8638 - val_loss: 0.6040 - val_accuracy: 0.8568\n",
      "Epoch 13/20\n",
      "644/644 [==============================] - 0s 755us/step - loss: 0.4475 - accuracy: 0.8724 - val_loss: 0.5821 - val_accuracy: 0.8478\n",
      "Epoch 14/20\n",
      "644/644 [==============================] - 0s 747us/step - loss: 0.4176 - accuracy: 0.8819 - val_loss: 0.5971 - val_accuracy: 0.8697\n",
      "Epoch 15/20\n",
      "644/644 [==============================] - 0s 746us/step - loss: 0.4040 - accuracy: 0.8858 - val_loss: 0.5601 - val_accuracy: 0.8707\n",
      "Epoch 16/20\n",
      "644/644 [==============================] - 0s 752us/step - loss: 0.4077 - accuracy: 0.8869 - val_loss: 0.5511 - val_accuracy: 0.8680\n",
      "Epoch 17/20\n",
      "644/644 [==============================] - 0s 753us/step - loss: 0.3823 - accuracy: 0.8940 - val_loss: 0.4999 - val_accuracy: 0.8780\n",
      "Epoch 18/20\n",
      "644/644 [==============================] - 0s 747us/step - loss: 0.3819 - accuracy: 0.8953 - val_loss: 0.5558 - val_accuracy: 0.8722\n",
      "Epoch 19/20\n",
      "644/644 [==============================] - 0s 762us/step - loss: 0.3616 - accuracy: 0.8994 - val_loss: 0.5440 - val_accuracy: 0.8749\n",
      "Epoch 20/20\n",
      "644/644 [==============================] - 0s 766us/step - loss: 0.3501 - accuracy: 0.9061 - val_loss: 0.5246 - val_accuracy: 0.8841\n",
      "394/394 [==============================] - 0s 414us/step - loss: 0.4795 - accuracy: 0.8877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4794556796550751, 0.8876984119415283]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(30,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,y_train,validation_split=0.3, epochs=20)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar pattern for the model with one hidden layer. When we used 50 neurons we got much better loss and accuracy. Also one thing to note is the models didnt take as long to run when using fewer neurons which once again comes back to the problem of trying to balance run time and good performance. Going off this you would presume that when we use more neurons the accuracy and loss would be better in both cases but for argument sake lets have a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using More Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "644/644 [==============================] - 1s 2ms/step - loss: 3.5550 - accuracy: 0.7864 - val_loss: 1.1000 - val_accuracy: 0.8415\n",
      "Epoch 2/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.6912 - accuracy: 0.8832 - val_loss: 0.7375 - val_accuracy: 0.8760\n",
      "Epoch 3/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4059 - accuracy: 0.9131 - val_loss: 0.5743 - val_accuracy: 0.8991\n",
      "Epoch 4/20\n",
      "644/644 [==============================] - 1s 969us/step - loss: 0.3227 - accuracy: 0.9276 - val_loss: 0.4924 - val_accuracy: 0.9046\n",
      "Epoch 5/20\n",
      "644/644 [==============================] - 1s 933us/step - loss: 0.2740 - accuracy: 0.9355 - val_loss: 0.5044 - val_accuracy: 0.9084\n",
      "Epoch 6/20\n",
      "644/644 [==============================] - 1s 973us/step - loss: 0.2435 - accuracy: 0.9412 - val_loss: 0.4461 - val_accuracy: 0.9156\n",
      "Epoch 7/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2123 - accuracy: 0.9476 - val_loss: 0.3870 - val_accuracy: 0.9238\n",
      "Epoch 8/20\n",
      "644/644 [==============================] - 1s 972us/step - loss: 0.2022 - accuracy: 0.9491 - val_loss: 0.3735 - val_accuracy: 0.9234\n",
      "Epoch 9/20\n",
      "644/644 [==============================] - 1s 898us/step - loss: 0.1960 - accuracy: 0.9518 - val_loss: 0.4269 - val_accuracy: 0.9153\n",
      "Epoch 10/20\n",
      "644/644 [==============================] - 1s 995us/step - loss: 0.1743 - accuracy: 0.9548 - val_loss: 0.3765 - val_accuracy: 0.9361\n",
      "Epoch 11/20\n",
      "644/644 [==============================] - 1s 902us/step - loss: 0.1680 - accuracy: 0.9575 - val_loss: 0.3285 - val_accuracy: 0.9398\n",
      "Epoch 12/20\n",
      "644/644 [==============================] - 1s 944us/step - loss: 0.1431 - accuracy: 0.9630 - val_loss: 0.3688 - val_accuracy: 0.9306\n",
      "Epoch 13/20\n",
      "644/644 [==============================] - 1s 994us/step - loss: 0.1427 - accuracy: 0.9633 - val_loss: 0.3709 - val_accuracy: 0.9401\n",
      "Epoch 14/20\n",
      "644/644 [==============================] - 1s 906us/step - loss: 0.1299 - accuracy: 0.9651 - val_loss: 0.3249 - val_accuracy: 0.9389\n",
      "Epoch 15/20\n",
      "644/644 [==============================] - 1s 953us/step - loss: 0.1128 - accuracy: 0.9692 - val_loss: 0.2988 - val_accuracy: 0.9410\n",
      "Epoch 16/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1187 - accuracy: 0.9691 - val_loss: 0.3209 - val_accuracy: 0.9434\n",
      "Epoch 17/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.0965 - accuracy: 0.9739 - val_loss: 0.3369 - val_accuracy: 0.9484\n",
      "Epoch 18/20\n",
      "644/644 [==============================] - 1s 888us/step - loss: 0.1176 - accuracy: 0.9695 - val_loss: 0.3207 - val_accuracy: 0.9448\n",
      "Epoch 19/20\n",
      "644/644 [==============================] - 1s 909us/step - loss: 0.0846 - accuracy: 0.9772 - val_loss: 0.3219 - val_accuracy: 0.9508\n",
      "Epoch 20/20\n",
      "644/644 [==============================] - 1s 984us/step - loss: 0.0669 - accuracy: 0.9790 - val_loss: 0.3467 - val_accuracy: 0.9518\n",
      "394/394 [==============================] - 0s 476us/step - loss: 0.2829 - accuracy: 0.9526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2829357981681824, 0.9526190757751465]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(70,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(70,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,y_train,validation_split=0.3, epochs=20)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To no surprise both the accuracy and loss is much better when we used 70 neurons. But also take a look at the time it took to run the model. Take a look at the model with two hidden layers that we ran for 20 epochs using 30 neurons in each hidden layer. The max run time of the 20 epochs is 953us while the max run time of our model with two hidden layers run for 20 epochs but with 70 neurons is 2ms. This takes more than twice the time. Although in our case the difference is barely noticeable when it is being run but let's say for example we are running a model with thousands of hidden layers. It could be the difference between one epoch taking 2 hours to run compared to an hour. Let's now take a look at the output we get when we test a model with only one hidden layer and for 20 epochs and with 70 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 5.9582 - accuracy: 0.7497 - val_loss: 1.0621 - val_accuracy: 0.7746\n",
      "Epoch 2/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.7361 - accuracy: 0.8299 - val_loss: 0.7731 - val_accuracy: 0.8387\n",
      "Epoch 3/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4908 - accuracy: 0.8765 - val_loss: 0.6423 - val_accuracy: 0.8667\n",
      "Epoch 4/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4069 - accuracy: 0.8925 - val_loss: 0.5797 - val_accuracy: 0.8790\n",
      "Epoch 5/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3562 - accuracy: 0.9034 - val_loss: 0.5242 - val_accuracy: 0.8895\n",
      "Epoch 6/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3056 - accuracy: 0.9191 - val_loss: 0.5168 - val_accuracy: 0.8949\n",
      "Epoch 7/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.3007 - accuracy: 0.9196 - val_loss: 0.4860 - val_accuracy: 0.9006\n",
      "Epoch 8/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2679 - accuracy: 0.9285 - val_loss: 0.4662 - val_accuracy: 0.9109\n",
      "Epoch 9/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2614 - accuracy: 0.9292 - val_loss: 0.4844 - val_accuracy: 0.9087\n",
      "Epoch 10/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2386 - accuracy: 0.9334 - val_loss: 0.4800 - val_accuracy: 0.9091\n",
      "Epoch 11/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2169 - accuracy: 0.9381 - val_loss: 0.4315 - val_accuracy: 0.9201\n",
      "Epoch 12/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2083 - accuracy: 0.9419 - val_loss: 0.3829 - val_accuracy: 0.9181\n",
      "Epoch 13/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1941 - accuracy: 0.9462 - val_loss: 0.4027 - val_accuracy: 0.9253\n",
      "Epoch 14/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1796 - accuracy: 0.9491 - val_loss: 0.4273 - val_accuracy: 0.9210\n",
      "Epoch 15/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1976 - accuracy: 0.9470 - val_loss: 0.3604 - val_accuracy: 0.9234\n",
      "Epoch 16/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1693 - accuracy: 0.9547 - val_loss: 0.4469 - val_accuracy: 0.9270\n",
      "Epoch 17/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1724 - accuracy: 0.9525 - val_loss: 0.4236 - val_accuracy: 0.9260\n",
      "Epoch 18/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1628 - accuracy: 0.9572 - val_loss: 0.4267 - val_accuracy: 0.9239\n",
      "Epoch 19/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1818 - accuracy: 0.9535 - val_loss: 0.4084 - val_accuracy: 0.9293\n",
      "Epoch 20/20\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1549 - accuracy: 0.9586 - val_loss: 0.4465 - val_accuracy: 0.9206\n",
      "394/394 [==============================] - 0s 726us/step - loss: 0.4020 - accuracy: 0.9276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40199121832847595, 0.9276190400123596]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(70,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,y_train,validation_split=0.3, epochs=20)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To no surprise we get a better loss and a better accuracy when using more neurons. When it comes to run time there isn't much of a difference but as I mentioned when this is run for large models then the difference will be noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, to get a model which gives the best performance is quite trivial. From our examples we could say that you should use a high number of epochs combined with a high number of neurons per layer and also a high number of hidden layers but in reality there is no point in having a model if it takes way too long to run. Therefore it is essential to be able to find somewhat of a 'happy medium' i.e. a model which doesn't take ages to run but also gives a good performance. I mentioned one method which can help that(EarlyStopping) but there are many more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
