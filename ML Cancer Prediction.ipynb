{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataframe\n",
    "df = pd.read_csv('illness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plas_gl</th>\n",
       "      <th>bp</th>\n",
       "      <th>result</th>\n",
       "      <th>skin_th</th>\n",
       "      <th>preg</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>ped</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>64</td>\n",
       "      <td>positive</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>35.1</td>\n",
       "      <td>0.692</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>74</td>\n",
       "      <td>negative</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.527</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>negative</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.597</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119</td>\n",
       "      <td>64</td>\n",
       "      <td>negative</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>34.9</td>\n",
       "      <td>0.725</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162</td>\n",
       "      <td>76</td>\n",
       "      <td>positive</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>53.2</td>\n",
       "      <td>0.759</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   plas_gl  bp    result  skin_th  preg  insulin   bmi    ped  age\n",
       "0      122  64  positive       32     1      156  35.1  0.692   30\n",
       "1       80  74  negative       11     1       60  30.0  0.527   22\n",
       "2      100  70  negative       26     0       50  30.8  0.597   21\n",
       "3      119  64  negative       18     0       92  34.9  0.725   23\n",
       "4      162  76  positive       56     0      100  53.2  0.759   25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect head of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.83      0.91      0.87        80\n",
      "   positive       0.72      0.55      0.62        33\n",
      "\n",
      "avg / total       0.80      0.81      0.80       113\n",
      "\n",
      "0.8053097345132744\n"
     ]
    }
   ],
   "source": [
    "#without scaling\n",
    "X = df.drop('result',1).values\n",
    "y = df['result'].values\n",
    "\n",
    "#split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=40)\n",
    "\n",
    "#Instantiate logistic regression classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#fit the classifier on the data\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#print out results\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(logreg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firstly talk about the confusion matrix. Precision is defined as the proportion of positive identifications that were actually identified as correct. Recall is define as the proportion of actual positives that are identified correctly. F1 score takes into account both recall and precision and gives us a measure of the preciseness and robustness of the model. Classification reports are often used to analyse a models performance although I will not specifically be mentioning it in this assignment I will still include it in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above the accuracy is 80.53%. Let's see if we can improve this accuracy. One method we could try regularisation. Regularisation would be a good technique to use here as the range of the data differs greatly. In the code below we will find an optimal value of C and which normalisation technique we should use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l1\n",
      "Best C: 1.623776739188721\n"
     ]
    }
   ],
   "source": [
    "#import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Instaniate the logistic regression classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#defining parameter grid \n",
    "penalty = ['l1','l2']\n",
    "C = np.logspace(-4,4,20)\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "clf = GridSearchCV(logreg,hyperparameters,cv=5)\n",
    "\n",
    "#fit grid search to data\n",
    "best_model = clf.fit(X,y)\n",
    "\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the optimal penalty is 'l1' and the optimal value for C = 1.62 . Let's update the parameters to their optimals and see how this effects the accuracy. Remember this data will need to be scaled to avoid bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.83      0.91      0.87        80\n",
      "   positive       0.72      0.55      0.62        33\n",
      "\n",
      "avg / total       0.80      0.81      0.80       113\n",
      "\n",
      "0.8407079646017699\n"
     ]
    }
   ],
   "source": [
    "#Import Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Instantiate the logistic regression classifier\n",
    "logreg = LogisticRegression(penalty = 'l1', C = 1.62)\n",
    "\n",
    "#Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=40)\n",
    "\n",
    "#scale the data\n",
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_test = sc_x.transform(X_test)\n",
    "\n",
    "#Fit it to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#compute and print accuracy\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(logreg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After both tuning and using the optimal parameters the accuracy has increased from 80.5% to 84.07%. Let's take a look as a precaution to see if the data is overfitted or underfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8407079646017699\n",
      "0.7756653992395437\n"
     ]
    }
   ],
   "source": [
    "print(logreg.score(X_test,y_test))\n",
    "print(logreg.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy on the test data and the training data is similar and therefore the data is neither underfitted nor overfitted. This is no surprise as l1 regularisation in itself is a solution to overfitting. Let's finally calculate a 95% confidence interval for the accuracy in order to get a better idea of the variance of this accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6947295208346654, 0.8780258971219909]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate mean of cross validation accuracies\n",
    "p = np.mean(cross_val_score(logreg,X,y,cv=20))\n",
    "\n",
    "#calculate upper and lower bound for confidence interval\n",
    "ci_upper = p + np.sqrt((p*(1-p))/20)\n",
    "ci_lower = p - np.sqrt((p*(1-p))/20)\n",
    "\n",
    "#append confidence interval to list and print it\n",
    "confidence_interval = [ci_lower,ci_upper]\n",
    "confidence_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that we are 95% confident that the true acuracy lies between 69.47% and 87.80% . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firstly test a KNN Classifier on the data without doing any scaling of the data or any optimisation of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65 13]\n",
      " [17 18]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.79      0.83      0.81        78\n",
      "   positive       0.58      0.51      0.55        35\n",
      "\n",
      "avg / total       0.73      0.73      0.73       113\n",
      "\n",
      "0.7345132743362832\n"
     ]
    }
   ],
   "source": [
    "#Import Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#Instantiate the classifier and fit it on the training data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "#compute and print accuracy\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see without tuning any parameters or scaling the data we get an accuracy of 72.56%. In the above example n_neighbors = 5 which is set by default. Let's now find an optimal value for n_neighbors and see if this improves the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metric: euclidean\n",
      "Best n_neighbors: 7\n",
      "Best Leaf Size: 10\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the KNeighborsClassifier classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#defining parameter grid\n",
    "metrics = ['euclidean','minkowski','manhattan']\n",
    "neighbors = np.arange(1,16)\n",
    "leaf_size = np.arange(10,100,10)\n",
    "param_grid = dict(metric=metrics, n_neighbors=neighbors, leaf_size=leaf_size)\n",
    "\n",
    "\n",
    "#Completing grid search on parameters given\n",
    "grid_search = GridSearchCV(knn,param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "#Print optimal values\n",
    "print('Best Metric:', grid_search.best_estimator_.get_params()['metric'])\n",
    "print('Best n_neighbors:', grid_search.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Leaf Size:', grid_search.best_estimator_.get_params()['leaf_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the best metric is 'euclidean' , the best neighbors = 15 and the best leaf size is 10. Let's now use these in our model to see if we can improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.80      0.87      0.83        78\n",
      "   positive       0.64      0.51      0.57        35\n",
      "\n",
      "avg / total       0.75      0.76      0.75       113\n",
      "\n",
      "0.7610619469026548\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the KNeighborsClassifier classifier\n",
    "knn = KNeighborsClassifier(metric = 'euclidean', n_neighbors = 7,leaf_size=10)\n",
    "\n",
    "#Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#Fit classifier on training data\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "#compute and print accuracy\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we get an accuracy of 76.11% which is an improvement on the original acuracy we got for the knn model. Let's now check if scaling the data will improve accuracy. Lets check which scaler gives us the best score starting with StandardScaler(Standard Scaler removes the mean and scales the data to unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.79      0.87      0.83        78\n",
      "   positive       0.63      0.49      0.55        35\n",
      "\n",
      "avg / total       0.74      0.75      0.74       113\n",
      "\n",
      "0.7522123893805309\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors = 7, leaf_size=10)\n",
    "\n",
    "#Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Fit on training data\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "#compute and print accuracy\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see after scaling and with optimal parameters we got an accuracy score of 75.2% which is a worse accuracy score without scaling so let's now check how RobustScaler affects the accuracy(Robust Scalers centering and scaling statistics are based on percentiles and are therefore not influenced by a few number of very large marginal outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.78      0.85      0.81        78\n",
      "   positive       0.57      0.46      0.51        35\n",
      "\n",
      "avg / total       0.71      0.73      0.72       113\n",
      "\n",
      "0.7256637168141593\n"
     ]
    }
   ],
   "source": [
    "#Import RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Instantiate the KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors = 7, leaf_size=10)\n",
    "\n",
    "#Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#Scale the data\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Fit on training data\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "#compute and print accuracy\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy for RobustScaler is 72.57% which is worse than the accuracy from StandardScaler, let's finally check how MinMax Scaler effects accuracy(MinMax Scaler rescales the data set such that all feature values are in the range [0,1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.79      0.90      0.84        78\n",
      "   positive       0.67      0.46      0.54        35\n",
      "\n",
      "avg / total       0.75      0.76      0.75       113\n",
      "\n",
      "0.7610619469026548\n"
     ]
    }
   ],
   "source": [
    "#Import MinMax Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Instantiate the KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors = 7, leaf_size=10)\n",
    "\n",
    "#Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Fit on training data\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "#compute and print accuracy\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see when we scale our data using MinMaxScaler we achieve an acurracy of 76.11% which is the highest accuracy we have received so far from the various scaling algorithms we used. This accuracy is also the same as the accuracy we got before we scaled the data. This is because scaling the data does not always improve the accuracy and in some cases it can even decrease the accuracy. Let's explore this more by computing a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6169917664178473, 0.8183023512292114]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.mean(cross_val_score(knn,X,y,cv=20))\n",
    "ci_upper = p + np.sqrt((p*(1-p))/20)\n",
    "ci_lower = p - np.sqrt((p*(1-p))/20)\n",
    "confidence_interval = [ci_lower,ci_upper]\n",
    "confidence_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confidence interval, we can say that we are 95% confident that the true accuracy lies between 61.70% and 81.83% .  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
